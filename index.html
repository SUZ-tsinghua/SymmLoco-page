<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Leveraging Symmetry in RL-based Legged Locomotion Control</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="./static/images/favicon.png" alt="System Introduction" style="width: 5%;">Leveraging Symmetry in RL-based Legged Locomotion Control</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/SUZ-tsinghua">Zhi Su</a><sup>*, 2</sup>,</span>
            <span class="author-block">
              <a href="">Xiaoyu Huang</a><sup>*, 1</sup>,</span>
            <span class="author-block">
              <a href="https://daniel-ordonez-apraez.netlify.app/">Daniel Ordo√±ez-Apraez</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://irisli17.github.io/">Yunfei Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://zyliatzju.github.io/">Zhongyu Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://qiayuanl.netlify.app/">Qiayuan Liao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/giulioturrisi">Giulio Turrisi</a><sup>3</sup>,
            </span>
            <span class="author-block"></span>
              <a href="">Massimiliano Pontil</a><sup>3</sup>,
            </span>
            <span class="author-block"></span>
              <a href="">Claudio Semini</a><sup>3</sup>,
            </span>
            <span class="author-block"></span>
              <a href="">Yi Wu</a><sup>2, 4</sup>,
            </span>
            <span class="author-block"></span>
              <a href="">Koushil Sreenath</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <div><span class="author-block"><sup>1</sup>UC Berkeley</span></div>
            <div><span class="author-block"><sup>2</sup>Institute for Interdisciplinary Information Sciences, Tsinghua University</span></div>
            <div><span class="author-block"><sup>3</sup>Istituto Italiano di Tecnologia, Italy</span></div>
            <div><span class="author-block"><sup>4</sup>Shanghai Qi Zhi Institute</span></div>
            <div><b>IROS 2024</b></div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.17320"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=Ad1clt4Yi4U"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/SUZ-tsinghua/SymmLoco"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<div style="text-align: center;">
  <img src="./static/images/cover.png" alt="System Introduction" style="width: 50%;">
</div>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, 
            but faces exploration difficulty without information about the robot's morphology. 
            The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. 
            This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, 
            such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. 
            To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance / invariance constraints. 
            We investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant / invariant, 
            and leveraging data augmentation to approximate equivariant / invariant actor-critics. 
            We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. 
            We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation. 
            Additionaly, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot to hardware.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/Ad1clt4Yi4U"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>

<!-- Poster. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Poster</h2>
    <!-- <div class="publication-video"> -->
      <object data="./static/IROS_poster.pdf" type="application/pdf" width="80%" height="1000px"></object>
    <!-- </div> -->
  </div>
</div>
<!--/ Poster. -->

</div>
</section>




<section class="section">
  <div class="container">
      <h2 class="title is-3" style="text-align: center;">Results</h2>
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <div class="content has-text-justified">
                  <p>
                    We compare PPOaug, PPOeqic, and a baseline PPO on four different tasks.
                    <!-- <ul>
                      <li><strong>Door Pushing</strong>: the quadrupedal robot needs to open a door using its front limbs while standing and adjust for doors that open either left or right.</li>
                      <li><strong>Dribbling</strong>: the quadrupedal robot needs to dribble a ball at given speeds and directions.</li>
                      <li><strong>Stand Turning</strong>: the quadrupedal robot needs to stand on its rear legs and turn around.</li>
                      <li><strong>Slope Walking</strong>: the quadrupedal robot needs to traverse a slope on its rear legs.</li>
                    </ul> -->
                  </p>
              </div>
          </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-three-fifths is-centered has-text-centered">
          <figure>
            <img src="./static/images/tasks.png" alt="tasks" style="width: 90%;">
          </figure>            
        </div>
      </div>
      
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <div class="content has-text-justified">
                  <p>
                     <p><strong>Training Curves</strong></p>
                      <!-- We compare the performance of the three methods in terms of sample efficiency and training return. The results indicate that PPOeqic consistently outperforms the other methods in sample efficiency and task performance. -->
                  </p>
              </div>
          </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-three-fifths is-centered has-text-centered">
          <figure>
            <img src="./static/images/curves.png" alt="tasks" style="width: 100%;">
            <figcaption>Comparison of training curves of PPO, PPOaug, and PPOeqic on four tasks from left to right: <strong>Door Pushing</strong>, <strong>Dribbling</strong>, <strong>Stand Turning</strong>
              , and <strong>Slope Walking</strong>. Learning curves show mean episodic return and standard deviation for three seeds. PPOeqic consistently
              demonstrates the highest training returns and sample efficiency in all tasks.</figcaption>
          </figure>            
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>Door Pushing</strong></p>
                    <!-- For the Door Pushing task, we calculate the success rate (SR) and the symmetric index (SI) of the learned policies. SI assesses the disparity between left and right scenarios, defined as \( \frac{2|X_R-X_L|}{X_R+X_L} \times 100\% \).
                    Here, \( X \) is any scalar metric (e.g. SR). We also perform out of distribution (OOD) evaluation by randomizing the robot's initial states.
                     The results show that PPOeqic achieves the highest SR and the lowest SI in both normal and OOD scenarios, indicating that the learned policy is more robust and symmetric. -->
                </p>
            </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-three-fifths is-centered has-text-centered">
          <figure>
            <img src="./static/images/door_pushing.png" alt="tasks" style="width: 100%;">
            <figcaption>Comparison of success rates (SR) and their symmetry index on <strong>Door Pushing</strong> tasks on training-distribution and out-of-distribution
              scenarios. Of the three variants, PPOeqic demonstrates both higher success rate and better symmetry index in both cases, indicating a
              better task-level symmetric policy</figcaption>
          </figure>            
        </div>
      </div>

      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>Dribbling</strong></p>
                    Incorporating symmetry in Dribbling task also results in better performance.
                    Qualitatively, PPOeqic and PPOaug keep the ball closer to the robot, while PPO often kicks it away and chases it. Quantitatively, the hard-constraint PPOeqic achieves an average
                    episodic return of 431.86 \( \pm \) 1.77, slightly outperforming PPO (427.02 \( \pm \) 2.15) and PPOaug (430.49 \( \pm \) 3.45), suggesting all methods are near saturation in the simulation environment.
                </p>
            </div>
        </div>
      </div> -->

      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>Stand Turning</strong></p>
                    <!-- <p>Besides loco-manipulation, we examine how symmetry helps if the task involves bipedal locomotion only.</p>
                    <p>Qualitatively, the PPO policy repeatedly develops a staggered gait with one foot positioned ahead of the other to maintain balance.
                    This gait results in significant jittering and hinders symmetric behaviors between left and right turns. In contrast, symmetry-incorporated
                    policies exhibit both symmetric gaits and more importantly symmetric turning trajectories in the task level.</p>
                    <p>Quantitatively, to measure the optimality of the policies, we gauge the error between the robot's commanded and actual headings. Additionally, we include the Cost of Transport (CoT) to evaluate the energy efficiency of the controller.
                    The results show that PPOeqic achieves the lowest heading error and CoT, indicating that the learned policy is more optimal and energy-efficient.</p> -->
                </p>
            </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-three-fifths is-centered has-text-centered">
          <figure>
            <img src="./static/images/stand_turning.png" alt="tasks" style="width: 100%;">
            <figcaption>Comparison of command tracking error on <strong>Stand Turning</strong> task, Cost of
              Transport and their symmetry index on stand turning tasks for
              three PPO variants. PPOeqic demonstrates less error and energy
              consumption, indicating a more optimal policy</figcaption>
          </figure>            
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>Slope Walking</strong></p>
                    <!-- <p>In this task, PPO shows significant issues with learning a symmetric foot placement, resulting in backward steps and frequent balance loss. 
                    This unregulated gait pattern leads to significant decrement in velocity tracking performance, where the policy walks only half as far as PPOeqic in the same timeframe.</p>
                    <p>The PPOaug policy improves upon PPO's limitations, exhibiting better alternation between the leading foot and more regulated foot placement.
                      However, the variations in step size and occasional staggering of one leg still indicate room for improvement.</p>
                    <p>On the other hand, PPOeqic presents the most stable gait. It maintains consistent foot exchange, regulated contact sequences, and similar step sizes.</p> -->
                </p>
            </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-three-fifths is-centered has-text-centered">
          <figure>
            <img src="./static/images/slope_walking.png" alt="tasks" style="width: 100%;">
            <figcaption>Plots of the feet positions in the desired walk direction.
              We observe that vanilla PPO learns an unstable step pattern with
              backward steps and foot slipping, resulting in 50% slower walking
              speed. PPOaug improves drastically but asymmetric patterns such
              as foot dragging still exists. PPOeqic presents the most symmetric
              interweaving gait pattern and walks at the desired speed.</figcaption>
          </figure>            
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>Real World Experiments</strong></p>
                    <p>We deploy the learned policies of Stand Turning tasks on the real-world quadrupedal robot CyberDog 2. The policy trained by PPOaug shows incredible robustness.
                      </p>
                </p>
            </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-three-fifths is-centered has-text-centered">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/a1rbdB16Zkk"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>          
        </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container">
      <h2 class="title is-3" style="text-align: center;">Background & Method</h2>
      <!-- <h2 class="title is-4" style="text-align: center;">Morphological Symmetry</h2> -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <div class="content has-text-justified">
                  <p>
                    <p><strong>Morphological Symmetry</strong></p>
                    We will study the robot's morphological symmetry using the principles of group theory. We only focus on the reflection symmetry group, denoted as \( \mathbb{G}:=\mathbb{C}_2=\{e,g_s|g_s^2=e\} \).
                    For any morphological configuration \( x \) of the quadrupedal robot, \( g_s \triangleright x\) gives the reflected configuration of \( x \) with respect to the sagittal plane.
                    Such symmetric group action can be applied to the task MDP's state space, action space, and observation space similarly.
                  </p>
              </div>
          </div>
      </div>
      <div class="columns is-centered">
          <div class="column is-three-fifths is-centered has-text-centered">
            <figure>
              <img src="./static/videos/go1-C2-symmetries_anim_static.gif" alt="morphological symmetry" style="width: 70%;">
              <figcaption>reflection symmetry, gif borrowed from <a href="https://github.com/Danfoa/MorphoSymm/blob/main/docs/static/animations/go1-C2-symmetries_anim_static.gif" target="_blank" rel="noopener noreferrer">MorphoSymm</a></figcaption>
            </figure>            
          </div>
      </div>
      <!-- <h2 class="title is-4" style="text-align: center;">Equivariant / Invariant Functions</h2> -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <div class="content has-text-justified">
                  <p>
                    <p><strong>Equivariant / Invariant Functions</strong></p>
                      A function \( f: \mathcal{X} \rightarrow \mathcal{Y} \) is <strong>equivariant</strong> with respect to a group action \( g \) if \( f(g \triangleright x) = g \triangleright f(x) \) for all \( x \in \mathcal{X} \).
                      It is <strong>invariant</strong> if \( f(g \triangleright x) = f(x) \) for all \( x \in \mathcal{X} \).
                  </p>
              </div>
          </div>
      </div>

      <!-- <h2 class="title is-4" style="text-align: center;">Symmetric MDP</h2> -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <div class="content has-text-justified">
                  <p>
                    <p><strong>Symmetric MDP</strong></p>
                      We call an MDP \( (\mathcal{S}, \mathcal{A}, r, T, p_0) \) symmetric if there exists a group \( \mathbb{G} \) acting on the state space \( \mathcal{S} \) and action space \( \mathcal{A} \) such that the reward function \( r \), transition function \( T \) and the density of initial states \( p_0 \) are invariant with respect to the group action.
                      \[ r(g_s \triangleright s, g_s \triangleright a) = r(s, a), \quad T(g_s \triangleright s, g_s \triangleright a, g_s \triangleright s') = T(s, a, s'), \quad p_0(g_s \triangleright s) = p_0(s) \]
                  </p>
              </div>
          </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                  <a href="https://www.cs.cmu.edu/~maz/publications/symmetry7.pdf" target="_blank" rel="noopener noreferrer">Previous study</a> has shown that symmetric MDPs possess \(\mathbb{G}\)-equivariant optimal
                  control policies and \(\mathbb{G}\)-invariant value functions.
                  \[ \pi(g_s \triangleright s) = g_s \triangleright \pi(s), \quad V(g_s \triangleright s) = V(s) \]
                  We aim to leverage this property to guide the exploration in policy learning using two approaches.
                </p>
            </div>
        </div>
    </div>

    <!-- <h2 class="title is-4" style="text-align: center;">PPOaug</h2> -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>PPOaug</strong>: PPO with data-augmentation.</p>
                    For each online collected transition tuples \((s, a, r, s')\), we apply the group action \(g_s\) to \((s,a,s')\), and then add the augmented transition tuple \((g_s \triangleright s, g_s \triangleright a, r, g_s \triangleright s')\) to the replay buffer.
                    The policy and value networks are trained both on the original and augmented transitions.
                </p>
            </div>
        </div>
    </div>

    <!-- <h2 class="title is-4" style="text-align: center;">PPOeqic</h2> -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
            <div class="content has-text-justified">
                <p>
                    <p><strong>PPOeqic</strong>: PPO with hard equivariance / invariance symmetry constraints on network architectures.</p>
                    By using the repositories <a href="https://github.com/QUVA-Lab/escnn" target="_blank" rel="noopener noreferrer">escnn</a> and <a href="https://github.com/Danfoa/MorphoSymm" target="_blank" rel="noopener noreferrer">MorphoSymm</a>, 
                    we enforce the policy network to be strictly equivariant and the value network to be strictly invariant to the group action \(g_s\).
                </p>
            </div>
        </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
      <h2 class="title is-3" style="text-align: center;">Acknowledgements</h2>
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <div class="content has-text-justified">
                  <p>
                    X. H., Z. L., Q. L., and K. S. acknowledge financial support from The AI Institute, 
                    InnoHK of the Government of the Hong Kong Special Administrative Region via the Hong Kong Centre for Logistics Robotics. 
                    G. T., M. P., and C. S. acknowledge financial support from PNRR MUR Project PE000013 "Future Artificial Intelligence Research", 
                    funded by the European Union - NextGenerationEU. The authors thank Prof. Xue Bin Peng for insightful discussions on this work. 
                    The authors also thank Xiaomi Inc. for providing CyberDog 2 for experiments. 
                  </p>
              </div>
          </div>
      </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{su2024leveraging,
    title={Leveraging Symmetry in RL-based Legged Locomotion Control},
    author={Su, Zhi and Huang, Xiaoyu and Ordo√±ez-Apraez, Daniel and Li, Yunfei and Li, Zhongyu and Liao, Qiayuan and Turrisi, Giulio and Pontil, Massimiliano and Semini, Claudio and Wu, Yi and Sreenath, Koushil},
    booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    year={2024},
    organization={IEEE}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <p>Page template borrowed from <a href="https://nerfies.github.io/"><span class="dnerf">Nerfies</span></a>.</p>
      </div>
  </div>
</footer>

</body>
</html>
